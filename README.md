# ğŸ¤Ÿ AI Sign Language Teacher
An interactive, real-time machine learning application designed to teach users hand signs and gestures using computer vision.

---

## ğŸŒŸ Project Overview
This project aims to bridge the communication gap by providing an automated way for anyone to learn sign language. By leveraging **MediaPipe** for hand tracking and **TensorFlow/Keras** for gesture classification, the application provides instant feedback on a user's signs compared to a reference library.

## ğŸš€ Key Features
*   **Real-Time Detection**: Low-latency hand landmark tracking using [MediaPipe Hands](https://google.github.io).
*   **Interactive Lessons**: Visual guides showing the target sign for users to mimic.
*   **Instant Feedback**: Visual cues (green/red overlays) to signal correct or incorrect gestures.
*   **Progress Tracking**: A simple gamified system to track signs mastered over time.

## ğŸ› ï¸ Tech Stack
*   **Language**: [Python 3.x](https://www.python.org)
*   **Computer Vision**: [OpenCV](https://opencv.org)
*   **Hand Tracking**: [MediaPipe](https://ai.google.dev)
*   **Machine Learning**: [TensorFlow](https://www.tensorflow.org) / [Scikit-learn](https://scikit-learn.org)
*   **UI/Dashboard**: [Streamlit](https://streamlit.io) (Planned)

## ğŸ“‚ Project Structure
```text
â”œâ”€â”€ data/               # Raw images and processed landmark CSVs
â”œâ”€â”€ models/             # Trained model weights (.h5, .tflite)
â”œâ”€â”€ notebooks/          # Exploratory Data Analysis & Prototyping
â”œâ”€â”€ src/                # Core source code
â”‚   â”œâ”€â”€ collection.py   # Webcam data capture script
â”‚   â”œâ”€â”€ processing.py   # Landmark extraction & normalization
â”‚   â”œâ”€â”€ train.py        # Model training logic
â”‚   â””â”€â”€ app.py          # Main application interface
â”œâ”€â”€ requirements.txt    # Project dependencies
â””â”€â”€ README.md           # You are here!
